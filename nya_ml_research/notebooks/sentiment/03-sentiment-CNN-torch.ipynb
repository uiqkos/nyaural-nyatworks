{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from operator import itemgetter, methodcaller\n",
    "from typing import Tuple, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hiddenlayer as hl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.engine import Engine\n",
    "from ignite.metrics import RunningAverage, Accuracy, Loss\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nya_ml import embeddings\n",
    "from nya_ml.preprocessing.tokenizer import Tokenizer\n",
    "from nya_ml_research.config import MODELS_PATH, DATA_PATH\n",
    "from nya_ml_research.src.models.ignitecnn import TextCNN\n",
    "from nya_ml_research.src.models.logreg import LogisticRegression\n",
    "from nya_utils.functools import identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "keyed_vectors = embeddings.get_source('ruwiki').load(MODELS_PATH / 'embeddings')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\uiqko\\pyenvs\\mint\\lib\\site-packages\\gensim\\models\\keyedvectors.py:478: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(keyed_vectors)\n",
    "tokenize = partial(tokenizer.tokenize, to=list, pad=50)\n",
    "vectorize = tokenizer.vectorize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "# weights = embedding.get_torch_tensor_embeddings()\n",
    "weights = torch.from_numpy(tokenizer.vectors.vectors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class SimpleConvolutionTextModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, embedding_dim: int, num_embeddings: int,\n",
    "                 embedding_weights: torch.Tensor):\n",
    "        super(SimpleConvolutionTextModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, _weight=embedding_weights)\n",
    "        self.embedding.requires_grad_(False)\n",
    "        self.conv_1 = nn.Conv1d(embedding_dim, hidden_size, 3, stride=3, bias=False)\n",
    "        self.pool = nn.MaxPool1d(3)\n",
    "        self.lin = nn.Linear(hidden_size * int(input_size / 3), output_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = x.float()\n",
    "\n",
    "        x = self.conv_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, x.size(1) * x.size(2))\n",
    "        x = self.lin(x)\n",
    "\n",
    "        y = torch.softmax(x, dim=1)\n",
    "\n",
    "        return y\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def eval(model, data, loss, batch_size, verbose=False):\n",
    "    test_dataloader = DataLoader(TensorDataset(*data), batch_size=batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            # print(X, y)\n",
    "            # print(torch.argmax(model(X).detach(), dim=1))\n",
    "            y_pred += model(X).detach()\n",
    "            y_true += y.detach() # torch.argmax(y, dim=1)\n",
    "    if verbose:\n",
    "        print(y_pred, y_true)\n",
    "\n",
    "    print('Loss:', loss.item())\n",
    "    if verbose:\n",
    "        print(classification_report(y_true, y_pred, digits=3))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH / 'raw' / 'ru-tweet-corp.csv', names=['text', 'label'], usecols=[4, 5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "df = shuffle(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                     text  label\n220374  В моей аудитории ни одна сучка даже не думала ...     -1\n134851   @NikiforovaValya ай яй( зачем с мамой ругаешься?     -1\n27330   @katyaryba будет тебе авг в личное пользование...      1\n137649  @LarinDmytro теперь рыги этот мусор сфотографи...     -1\n48391   УРААААА.. Я буду шефом на Тест-Драйв #УрФУ\\nее...      1\n56930   @Rus_Smash С днем рождения, Руслан!) Здоровья,...      1\n116297  Как же заебал телефон падать, все сенсорные те...     -1\n116545     он мне сегодня снился:( http://t.co/8GhbjIw3rM     -1\n31340   RT @KOA143: теперь,протянув максимум времени,м...      1\n28327   К черту фигуру, если есть нутелла) http://t.co...      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>220374</th>\n      <td>В моей аудитории ни одна сучка даже не думала ...</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>134851</th>\n      <td>@NikiforovaValya ай яй( зачем с мамой ругаешься?</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>27330</th>\n      <td>@katyaryba будет тебе авг в личное пользование...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>137649</th>\n      <td>@LarinDmytro теперь рыги этот мусор сфотографи...</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>48391</th>\n      <td>УРААААА.. Я буду шефом на Тест-Драйв #УрФУ\\nее...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>56930</th>\n      <td>@Rus_Smash С днем рождения, Руслан!) Здоровья,...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>116297</th>\n      <td>Как же заебал телефон падать, все сенсорные те...</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>116545</th>\n      <td>он мне сегодня снился:( http://t.co/8GhbjIw3rM</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>31340</th>\n      <td>RT @KOA143: теперь,протянув максимум времени,м...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>28327</th>\n      <td>К черту фигуру, если есть нутелла) http://t.co...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": " 1    114911\n-1    111923\nName: label, dtype: int64"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uiqko\\AppData\\Local\\Temp\\ipykernel_19416\\2312225336.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['label'][df.label == -1] = 0\n"
     ]
    }
   ],
   "source": [
    "df['label'][df.label == -1] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "1    114911\n0    111923\nName: label, dtype: int64"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "limit = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "X = df.text.head(limit)\n",
    "y = df.label.head(limit)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 300.31it/s]\n"
     ]
    }
   ],
   "source": [
    "X = X.progress_apply(tokenize)\n",
    "# y = y.progress_apply(lambda label: [label, 1 - label][::-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "X = np.array(X.tolist())\n",
    "y = np.array(y.tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X)\n",
    "y = torch.from_numpy(y).float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "data = train_test_split(X, y, test_size=0.2, stratify=y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "class ConvolutionTextModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, embedding_dim: int, num_embeddings: int,\n",
    "                 embedding_weights: torch.Tensor, kernel_sizes, drop_prob: float = 0.5):\n",
    "        super(ConvolutionTextModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, _weight=embedding_weights)\n",
    "        # self.embedding.requires_grad_(False)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(embedding_dim, hidden_size, k)\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.lin = nn.Linear(hidden_size * len(kernel_sizes), output_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        # print(x.size())\n",
    "        x = x.transpose(1, 2)\n",
    "        # print(x.size())\n",
    "        x = x.float()\n",
    "\n",
    "        x = [F.relu(conv(x)) for conv in self.convs]\n",
    "        # print(len(x), list(map(methodcaller('size'), x)))\n",
    "        x = [F.max_pool1d(x_, list(x_.size())[-1]) for x_ in x]\n",
    "        # print(len(x), list(map(methodcaller('size'), x)))\n",
    "        x = torch.cat(x, dim=1)\n",
    "        # print(x.size())\n",
    "        x = x.squeeze()\n",
    "        x = self.dropout(x)\n",
    "        x = self.lin(x)\n",
    "        # print(x.size())\n",
    "\n",
    "        return torch.softmax(x, 0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "model = ConvolutionTextModel(\n",
    "    input_size=50,\n",
    "    hidden_size=6,\n",
    "    output_size=2,\n",
    "    kernel_sizes=[2, 3, 4],\n",
    "    embedding_dim=300,\n",
    "    num_embeddings=len(tokenizer.vectors),\n",
    "    embedding_weights=weights\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "model = SimpleConvolutionTextModel(\n",
    "    input_size=50,\n",
    "    hidden_size=6,\n",
    "    output_size=2,\n",
    "    embedding_dim=300,\n",
    "    num_embeddings=len(tokenizer.vectors),\n",
    "    embedding_weights=weights\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "model = LogisticRegression(50, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "model = TextCNN(\n",
    "    vocab_size=len(tokenizer.vectors),\n",
    "    embedding_dim=tokenizer.vectors.vector_size,\n",
    "    kernel_sizes=[3, 4, 5],\n",
    "    num_filters=100,\n",
    "    num_classes=1,\n",
    "    d_prob=0.5,\n",
    "    embedding_weights=weights\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0., grad_fn=<SqueezeBackward0>)"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([tokenize('привет всем любителям пончиков')]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN(\n",
      "  (embedding): Embedding(249334, 300)\n",
      "  (conv): ModuleList(\n",
      "    (0): Conv1d(300, 100, kernel_size=(3,), stride=(1,))\n",
      "    (1): Conv1d(300, 100, kernel_size=(4,), stride=(1,))\n",
      "    (2): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:15,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931471824645996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:02<00:11,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931471824645996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:04<00:09,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931471824645996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:05<00:07,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931471824645996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:06<00:06,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931471824645996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:07<00:05,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931471824645996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:09<00:03,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931471824645996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:10<00:02,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931471824645996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:11<00:01,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931471824645996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:12<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931471824645996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model=model, data=data, epochs=10, learning_rate=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "[Parameter containing:\n tensor([[-5.7760, -1.4857,  1.4283,  ..., -0.9964, -1.0746, -0.8292],\n         [ 4.5155,  3.8298, -0.6947,  ..., -0.4361,  4.5493,  4.5197],\n         [-2.3417, -3.3157,  0.3458,  ...,  6.7954,  0.7102,  4.2465],\n         ...,\n         [-0.5283, -0.0311, -0.2655,  ...,  0.0096, -0.1814,  0.0298],\n         [-0.1142,  0.9447,  0.2502,  ..., -0.0167,  0.4876, -0.2878],\n         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]),\n Parameter containing:\n tensor([[[-8.1584, -9.0035,  8.9808],\n          [-9.5645, -9.0069,  8.9741],\n          [-8.8145, -9.0149, -9.0043],\n          ...,\n          [ 8.9228, -9.0034,  8.9753],\n          [-9.0200,  8.9720,  9.0322],\n          [-8.7616, -8.9886,  8.9721]],\n \n         [[ 9.0071,  8.9946, -8.9975],\n          [-9.0158, -8.9667,  8.9947],\n          [-9.0036, -9.0022,  8.9979],\n          ...,\n          [ 8.9793, -9.0294, -8.9816],\n          [ 9.0162, -9.0227, -8.9813],\n          [-8.9997,  8.9986, -8.9849]],\n \n         [[ 9.0178, -9.0004,  8.9995],\n          [-8.9970, -9.0064, -8.9888],\n          [-8.9349, -9.0055,  8.9844],\n          ...,\n          [ 8.9914, -8.9878,  8.9886],\n          [-8.9677,  8.9965,  8.9793],\n          [-9.0101, -9.0249,  9.0050]],\n \n         [[-9.1438,  9.1862, -9.0229],\n          [-8.7497, -9.0545, -8.9817],\n          [-9.0104, -9.2380,  8.9968],\n          ...,\n          [-8.5192, -8.9248, -9.0043],\n          [ 9.3530,  8.9753,  9.0277],\n          [-8.9364,  8.7671,  8.9869]],\n \n         [[-8.3980,  8.9949,  8.2380],\n          [-9.7769,  8.9201, -9.2811],\n          [ 9.1848, -8.9335,  9.3102],\n          ...,\n          [-8.9839,  9.2468,  6.7834],\n          [ 9.3595,  8.7924, -9.4970],\n          [-7.7782, -9.1246,  9.0603]],\n \n         [[ 9.4791, -8.9513, -9.1657],\n          [ 8.5880,  8.9436, -9.0805],\n          [ 9.2572,  9.1107, -9.0686],\n          ...,\n          [ 9.1324,  9.1064,  8.4051],\n          [-8.4525, -9.0624, -9.2539],\n          [ 9.3114, -9.0884, -8.9187]]], requires_grad=True),\n Parameter containing:\n tensor([[-9.0311e+00,  8.8985e+00,  9.0599e+00,  8.9804e+00,  9.0143e+00,\n           9.0801e+00,  8.9302e+00, -9.0734e+00, -6.4760e-02, -5.3875e-02,\n          -8.3591e-02,  3.4846e-02, -5.5159e-02, -3.1747e-02,  8.4583e-02,\n           9.2507e-02,  8.9349e+00,  9.0167e+00,  8.9651e+00,  9.0034e+00,\n           9.0081e+00,  8.9185e+00,  8.9289e+00,  8.9457e+00,  1.1838e-02,\n          -7.7277e-02, -7.4329e-02,  6.7493e-02,  1.2994e-02, -9.9205e-02,\n          -9.8228e-02,  1.0043e-01,  8.9203e+00,  8.9650e+00,  9.0870e+00,\n           8.9860e+00,  9.0069e+00, -8.9979e+00,  9.0008e+00,  9.0561e+00,\n           6.6548e-02,  9.4634e-02,  3.4775e-02,  9.0581e-02, -8.3238e-02,\n           2.5641e-02,  8.8967e-02,  6.2310e-02, -8.9955e+00,  8.9249e+00,\n           9.0302e+00, -8.9956e+00, -9.0592e+00, -9.0025e+00, -9.0445e+00,\n           9.0667e+00, -2.4325e-02,  3.6998e-02, -1.3856e-03, -8.7502e-02,\n           3.2464e-02, -9.6732e-02, -3.7552e-02, -2.3862e-02,  8.9499e+00,\n           8.9266e+00,  8.9059e+00,  9.0390e+00, -9.0308e+00,  9.0462e+00,\n          -8.9117e+00,  8.9623e+00,  3.5262e-03, -7.1548e-02, -7.2363e-02,\n           1.3333e-02,  4.5377e-03,  1.0156e-01, -3.5850e-02, -1.1703e-03,\n           9.0695e+00,  9.0864e+00,  9.0616e+00,  8.9820e+00, -9.0964e+00,\n           8.9623e+00, -8.9383e+00,  9.0191e+00, -3.9215e-02, -9.3515e-02,\n          -2.7995e-02,  5.9676e-02, -3.2842e-02,  8.1553e-02,  1.8525e-02,\n          -2.1421e-02],\n         [ 9.0768e+00, -8.9016e+00, -8.9444e+00, -6.6203e+00, -9.0089e+00,\n          -2.8926e+00, -7.4905e+00,  8.9507e+00,  2.7832e+00, -4.9626e-02,\n          -7.2434e-02, -9.1386e-02, -5.3478e-02, -5.1558e-02, -5.0505e-02,\n           6.9576e-02, -8.9226e+00, -8.9786e+00, -8.9395e+00, -8.9831e+00,\n          -8.9283e+00, -8.9832e+00, -8.9206e+00, -8.9311e+00,  2.4536e+00,\n          -1.0406e-02, -5.1325e-02,  4.6935e-02,  4.1341e-02, -4.0094e-02,\n          -3.9125e-02, -4.0733e-02, -8.9950e+00, -8.9490e+00, -8.9368e+00,\n          -8.9692e+00, -8.9373e+00,  9.0096e+00, -8.9738e+00, -9.0229e+00,\n          -3.7754e-02, -4.9097e-02,  3.3361e-02, -7.0958e-02,  2.8807e-02,\n           1.5617e-02, -8.9312e-02, -5.5919e-02,  8.9510e+00, -8.6753e+00,\n          -8.9849e+00,  8.9412e+00,  9.0243e+00,  8.9564e+00,  1.0893e+01,\n          -9.0200e+00,  1.2012e-02, -2.5468e-02, -5.3773e-02,  8.0731e-02,\n           5.1378e-02, -1.4550e-02,  8.9608e-02, -6.8145e-02, -8.9904e+00,\n          -4.4828e+00, -8.9948e+00, -6.4206e+00,  1.5156e+01, -3.3767e+00,\n           9.0294e+00, -9.0991e+00, -6.2801e-03,  1.3475e-02,  8.3095e-03,\n           8.2463e-02,  1.1425e-02,  5.8694e-02,  7.9296e-03, -3.5855e-02,\n          -8.9508e+00, -8.5599e+00, -7.7462e+00, -8.3184e+00,  1.7005e+01,\n          -7.3516e+00,  1.6159e+01, -8.9854e+00,  2.9949e+00, -8.3595e-02,\n          -1.9911e-02,  1.0081e-01,  1.0055e-01, -7.8121e-02, -8.4359e-02,\n           9.0718e-02]], requires_grad=True)]"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0)] [tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.)]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.493     1.000     0.660       986\n",
      "         1.0      0.000     0.000     0.000      1014\n",
      "\n",
      "    accuracy                          0.493      2000\n",
      "   macro avg      0.246     0.500     0.330      2000\n",
      "weighted avg      0.243     0.493     0.326      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\uiqko\\pyenvs\\mint\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\uiqko\\pyenvs\\mint\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\uiqko\\pyenvs\\mint\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "eval(model, (data[1], data[3]), None, 32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "from nya_ml_research.src.eval import Eval\n",
    "from nya_ml_research.src.step import Step"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10000 [00:00<?, ?it/s]\u001B[A\n",
      "  0%|          | 35/10000 [00:00<00:28, 346.50it/s]\u001B[A\n",
      "  1%|          | 81/10000 [00:00<00:24, 410.62it/s]\u001B[A\n",
      "  1%|▏         | 131/10000 [00:00<00:21, 449.15it/s]\u001B[A\n",
      "  2%|▏         | 176/10000 [00:00<00:22, 427.70it/s]\u001B[A\n",
      "  2%|▏         | 223/10000 [00:00<00:22, 442.38it/s]\u001B[A\n",
      "  3%|▎         | 268/10000 [00:00<00:22, 436.24it/s]\u001B[A\n",
      "  3%|▎         | 314/10000 [00:00<00:21, 440.97it/s]\u001B[A\n",
      "  4%|▎         | 359/10000 [00:00<00:23, 417.84it/s]\u001B[A\n",
      "  4%|▍         | 408/10000 [00:00<00:21, 439.10it/s]\u001B[A\n",
      "  5%|▍         | 453/10000 [00:01<00:21, 435.87it/s]\u001B[A\n",
      "  5%|▍         | 497/10000 [00:01<00:22, 416.27it/s]\u001B[A\n",
      "  5%|▌         | 544/10000 [00:01<00:21, 431.60it/s]\u001B[A\n",
      "  6%|▌         | 592/10000 [00:01<00:21, 444.36it/s]\u001B[A\n",
      "  6%|▋         | 637/10000 [00:01<00:22, 424.88it/s]\u001B[A\n",
      "  7%|▋         | 681/10000 [00:01<00:21, 429.17it/s]\u001B[A\n",
      "  7%|▋         | 725/10000 [00:01<00:22, 421.38it/s]\u001B[A\n",
      "  8%|▊         | 768/10000 [00:01<00:22, 413.18it/s]\u001B[A\n",
      "  8%|▊         | 810/10000 [00:01<00:22, 411.60it/s]\u001B[A\n",
      "  9%|▊         | 856/10000 [00:02<00:21, 425.53it/s]\u001B[A\n",
      "  9%|▉         | 900/10000 [00:02<00:21, 428.50it/s]\u001B[A\n",
      "  9%|▉         | 943/10000 [00:02<00:22, 399.63it/s]\u001B[A\n",
      " 10%|▉         | 989/10000 [00:02<00:21, 416.56it/s]\u001B[A\n",
      " 10%|█         | 1038/10000 [00:02<00:20, 436.35it/s]\u001B[A\n",
      " 11%|█         | 1089/10000 [00:02<00:19, 457.67it/s]\u001B[A\n",
      " 11%|█▏        | 1143/10000 [00:02<00:18, 480.35it/s]\u001B[A\n",
      " 12%|█▏        | 1192/10000 [00:02<00:18, 478.99it/s]\u001B[A\n",
      " 12%|█▏        | 1241/10000 [00:02<00:19, 450.44it/s]\u001B[A\n",
      " 13%|█▎        | 1287/10000 [00:03<00:21, 407.22it/s]\u001B[A\n",
      " 13%|█▎        | 1333/10000 [00:03<00:20, 419.02it/s]\u001B[A\n",
      " 14%|█▍        | 1376/10000 [00:03<00:20, 410.78it/s]\u001B[A\n",
      " 14%|█▍        | 1418/10000 [00:03<00:20, 409.93it/s]\u001B[A\n",
      " 15%|█▍        | 1460/10000 [00:03<00:21, 404.79it/s]\u001B[A\n",
      " 15%|█▌        | 1501/10000 [00:03<00:21, 402.85it/s]\u001B[A\n",
      " 15%|█▌        | 1545/10000 [00:03<00:20, 409.95it/s]\u001B[A\n",
      " 16%|█▌        | 1589/10000 [00:03<00:20, 415.00it/s]\u001B[A\n",
      " 16%|█▋        | 1633/10000 [00:03<00:19, 422.24it/s]\u001B[A\n",
      " 17%|█▋        | 1681/10000 [00:03<00:19, 435.33it/s]\u001B[A\n",
      " 17%|█▋        | 1733/10000 [00:04<00:18, 456.14it/s]\u001B[A\n",
      " 18%|█▊        | 1779/10000 [00:04<00:18, 448.09it/s]\u001B[A\n",
      " 18%|█▊        | 1824/10000 [00:04<00:18, 434.73it/s]\u001B[A\n",
      " 19%|█▊        | 1874/10000 [00:04<00:18, 450.89it/s]\u001B[A\n",
      " 19%|█▉        | 1920/10000 [00:04<00:18, 432.24it/s]\u001B[A\n",
      " 20%|█▉        | 1964/10000 [00:04<00:18, 425.99it/s]\u001B[A\n",
      " 20%|██        | 2010/10000 [00:04<00:18, 434.42it/s]\u001B[A\n",
      " 21%|██        | 2054/10000 [00:04<00:19, 408.95it/s]\u001B[A\n",
      " 21%|██        | 2098/10000 [00:04<00:18, 417.59it/s]\u001B[A\n",
      " 21%|██▏       | 2149/10000 [00:05<00:17, 442.60it/s]\u001B[A\n",
      " 22%|██▏       | 2195/10000 [00:05<00:17, 446.33it/s]\u001B[A\n",
      " 22%|██▏       | 2243/10000 [00:05<00:17, 454.84it/s]\u001B[A\n",
      " 23%|██▎       | 2289/10000 [00:05<00:17, 434.66it/s]\u001B[A\n",
      " 23%|██▎       | 2333/10000 [00:05<00:17, 430.07it/s]\u001B[A\n",
      " 24%|██▍       | 2377/10000 [00:05<00:17, 427.99it/s]\u001B[A\n",
      " 24%|██▍       | 2420/10000 [00:05<00:17, 427.35it/s]\u001B[A\n",
      " 25%|██▍       | 2465/10000 [00:05<00:17, 430.20it/s]\u001B[A\n",
      " 25%|██▌       | 2509/10000 [00:05<00:17, 431.79it/s]\u001B[A\n",
      " 26%|██▌       | 2553/10000 [00:05<00:17, 426.70it/s]\u001B[A\n",
      " 26%|██▌       | 2596/10000 [00:06<00:17, 421.49it/s]\u001B[A\n",
      " 26%|██▋       | 2641/10000 [00:06<00:17, 421.19it/s]\u001B[A\n",
      " 27%|██▋       | 2685/10000 [00:06<00:17, 422.99it/s]\u001B[A\n",
      " 27%|██▋       | 2728/10000 [00:06<00:17, 410.69it/s]\u001B[A\n",
      " 28%|██▊       | 2770/10000 [00:06<00:18, 397.41it/s]\u001B[A\n",
      " 28%|██▊       | 2810/10000 [00:06<00:18, 386.15it/s]\u001B[A\n",
      " 28%|██▊       | 2849/10000 [00:06<00:19, 375.61it/s]\u001B[A\n",
      " 29%|██▉       | 2890/10000 [00:06<00:18, 383.10it/s]\u001B[A\n",
      " 29%|██▉       | 2930/10000 [00:06<00:18, 386.83it/s]\u001B[A\n",
      " 30%|██▉       | 2970/10000 [00:07<00:18, 386.15it/s]\u001B[A\n",
      " 30%|███       | 3022/10000 [00:07<00:16, 423.70it/s]\u001B[A\n",
      " 31%|███       | 3069/10000 [00:07<00:15, 437.22it/s]\u001B[A\n",
      " 31%|███       | 3120/10000 [00:07<00:15, 455.95it/s]\u001B[A\n",
      " 32%|███▏      | 3166/10000 [00:07<00:15, 446.64it/s]\u001B[A\n",
      " 32%|███▏      | 3215/10000 [00:07<00:14, 459.28it/s]\u001B[A\n",
      " 33%|███▎      | 3262/10000 [00:07<00:14, 461.08it/s]\u001B[A\n",
      " 33%|███▎      | 3310/10000 [00:07<00:14, 466.66it/s]\u001B[A\n",
      " 34%|███▎      | 3359/10000 [00:07<00:14, 469.38it/s]\u001B[A\n",
      " 34%|███▍      | 3406/10000 [00:07<00:14, 465.42it/s]\u001B[A\n",
      " 35%|███▍      | 3453/10000 [00:08<00:14, 440.78it/s]\u001B[A\n",
      " 35%|███▍      | 3498/10000 [00:08<00:14, 443.39it/s]\u001B[A\n",
      " 35%|███▌      | 3544/10000 [00:08<00:14, 444.35it/s]\u001B[A\n",
      " 36%|███▌      | 3589/10000 [00:08<00:14, 444.70it/s]\u001B[A\n",
      " 36%|███▋      | 3637/10000 [00:08<00:14, 451.05it/s]\u001B[A\n",
      " 37%|███▋      | 3690/10000 [00:08<00:13, 468.71it/s]\u001B[A\n",
      " 37%|███▋      | 3737/10000 [00:08<00:13, 461.01it/s]\u001B[A\n",
      " 38%|███▊      | 3784/10000 [00:08<00:13, 455.62it/s]\u001B[A\n",
      " 38%|███▊      | 3834/10000 [00:08<00:13, 467.15it/s]\u001B[A\n",
      " 39%|███▉      | 3885/10000 [00:08<00:12, 476.91it/s]\u001B[A\n",
      " 39%|███▉      | 3940/10000 [00:09<00:12, 495.48it/s]\u001B[A\n",
      " 40%|███▉      | 3995/10000 [00:09<00:11, 511.50it/s]\u001B[A\n",
      " 40%|████      | 4047/10000 [00:09<00:11, 505.07it/s]\u001B[A\n",
      " 41%|████      | 4098/10000 [00:09<00:12, 489.24it/s]\u001B[A\n",
      " 42%|████▏     | 4151/10000 [00:09<00:11, 499.57it/s]\u001B[A\n",
      " 42%|████▏     | 4202/10000 [00:09<00:11, 488.40it/s]\u001B[A\n",
      " 43%|████▎     | 4253/10000 [00:09<00:11, 494.59it/s]\u001B[A\n",
      " 43%|████▎     | 4303/10000 [00:09<00:12, 468.87it/s]\u001B[A\n",
      " 44%|████▎     | 4351/10000 [00:09<00:12, 465.43it/s]\u001B[A\n",
      " 44%|████▍     | 4398/10000 [00:10<00:12, 460.20it/s]\u001B[A\n",
      " 44%|████▍     | 4445/10000 [00:10<00:12, 452.64it/s]\u001B[A\n",
      " 45%|████▍     | 4494/10000 [00:10<00:11, 461.97it/s]\u001B[A\n",
      " 45%|████▌     | 4541/10000 [00:10<00:12, 452.54it/s]\u001B[A\n",
      " 46%|████▌     | 4587/10000 [00:10<00:12, 445.72it/s]\u001B[A\n",
      " 46%|████▋     | 4633/10000 [00:10<00:11, 448.50it/s]\u001B[A\n",
      " 47%|████▋     | 4679/10000 [00:10<00:11, 446.67it/s]\u001B[A\n",
      " 47%|████▋     | 4726/10000 [00:10<00:11, 453.45it/s]\u001B[A\n",
      " 48%|████▊     | 4772/10000 [00:10<00:11, 452.72it/s]\u001B[A\n",
      " 48%|████▊     | 4818/10000 [00:10<00:11, 454.86it/s]\u001B[A\n",
      " 49%|████▊     | 4864/10000 [00:11<00:11, 451.03it/s]\u001B[A\n",
      " 49%|████▉     | 4912/10000 [00:11<00:11, 452.90it/s]\u001B[A\n",
      " 50%|████▉     | 4958/10000 [00:11<00:11, 448.43it/s]\u001B[A\n",
      " 50%|█████     | 5009/10000 [00:11<00:10, 466.41it/s]\u001B[A\n",
      " 51%|█████     | 5061/10000 [00:11<00:10, 479.34it/s]\u001B[A\n",
      " 51%|█████     | 5111/10000 [00:11<00:10, 481.20it/s]\u001B[A\n",
      " 52%|█████▏    | 5160/10000 [00:11<00:10, 467.28it/s]\u001B[A\n",
      " 52%|█████▏    | 5207/10000 [00:11<00:10, 466.72it/s]\u001B[A\n",
      " 53%|█████▎    | 5254/10000 [00:11<00:10, 439.46it/s]\u001B[A\n",
      " 53%|█████▎    | 5303/10000 [00:12<00:10, 451.16it/s]\u001B[A\n",
      " 54%|█████▎    | 5359/10000 [00:12<00:09, 480.87it/s]\u001B[A\n",
      " 54%|█████▍    | 5408/10000 [00:12<00:09, 473.91it/s]\u001B[A\n",
      " 55%|█████▍    | 5456/10000 [00:12<00:09, 471.58it/s]\u001B[A\n",
      " 55%|█████▌    | 5504/10000 [00:12<00:09, 469.93it/s]\u001B[A\n",
      " 56%|█████▌    | 5552/10000 [00:12<00:09, 464.75it/s]\u001B[A\n",
      " 56%|█████▌    | 5600/10000 [00:12<00:09, 467.82it/s]\u001B[A\n",
      " 56%|█████▋    | 5647/10000 [00:12<00:09, 459.07it/s]\u001B[A\n",
      " 57%|█████▋    | 5696/10000 [00:12<00:09, 463.98it/s]\u001B[A\n",
      " 57%|█████▋    | 5743/10000 [00:12<00:09, 448.75it/s]\u001B[A\n",
      " 58%|█████▊    | 5789/10000 [00:13<00:09, 449.43it/s]\u001B[A\n",
      " 58%|█████▊    | 5839/10000 [00:13<00:08, 462.49it/s]\u001B[A\n",
      " 59%|█████▉    | 5886/10000 [00:13<00:09, 437.87it/s]\u001B[A\n",
      " 59%|█████▉    | 5936/10000 [00:13<00:08, 452.84it/s]\u001B[A\n",
      " 60%|█████▉    | 5987/10000 [00:13<00:08, 467.85it/s]\u001B[A\n",
      " 60%|██████    | 6035/10000 [00:13<00:08, 446.79it/s]\u001B[A\n",
      " 61%|██████    | 6084/10000 [00:13<00:08, 455.11it/s]\u001B[A\n",
      " 61%|██████▏   | 6130/10000 [00:13<00:08, 450.07it/s]\u001B[A\n",
      " 62%|██████▏   | 6180/10000 [00:13<00:08, 460.41it/s]\u001B[A\n",
      " 62%|██████▏   | 6229/10000 [00:14<00:08, 466.29it/s]\u001B[A\n",
      " 63%|██████▎   | 6282/10000 [00:14<00:07, 484.79it/s]\u001B[A\n",
      " 63%|██████▎   | 6331/10000 [00:14<00:07, 477.93it/s]\u001B[A\n",
      " 64%|██████▍   | 6384/10000 [00:14<00:07, 491.69it/s]\u001B[A\n",
      " 64%|██████▍   | 6434/10000 [00:14<00:07, 475.96it/s]\u001B[A\n",
      " 65%|██████▍   | 6485/10000 [00:14<00:07, 484.36it/s]\u001B[A\n",
      " 65%|██████▌   | 6534/10000 [00:14<00:07, 473.58it/s]\u001B[A\n",
      " 66%|██████▌   | 6582/10000 [00:14<00:07, 474.06it/s]\u001B[A\n",
      " 66%|██████▋   | 6630/10000 [00:14<00:07, 463.66it/s]\u001B[A\n",
      " 67%|██████▋   | 6677/10000 [00:14<00:07, 457.57it/s]\u001B[A\n",
      " 67%|██████▋   | 6725/10000 [00:15<00:07, 461.36it/s]\u001B[A\n",
      " 68%|██████▊   | 6778/10000 [00:15<00:06, 479.91it/s]\u001B[A\n",
      " 68%|██████▊   | 6827/10000 [00:15<00:06, 480.05it/s]\u001B[A\n",
      " 69%|██████▉   | 6876/10000 [00:15<00:06, 473.24it/s]\u001B[A\n",
      " 69%|██████▉   | 6926/10000 [00:15<00:06, 478.25it/s]\u001B[A\n",
      " 70%|██████▉   | 6974/10000 [00:15<00:06, 473.22it/s]\u001B[A\n",
      " 70%|███████   | 7022/10000 [00:15<00:06, 461.73it/s]\u001B[A\n",
      " 71%|███████   | 7073/10000 [00:15<00:06, 472.92it/s]\u001B[A\n",
      " 71%|███████   | 7121/10000 [00:15<00:06, 457.68it/s]\u001B[A\n",
      " 72%|███████▏  | 7167/10000 [00:16<00:06, 455.43it/s]\u001B[A\n",
      " 72%|███████▏  | 7213/10000 [00:16<00:06, 453.12it/s]\u001B[A\n",
      " 73%|███████▎  | 7259/10000 [00:16<00:06, 447.35it/s]\u001B[A\n",
      " 73%|███████▎  | 7304/10000 [00:16<00:06, 428.29it/s]\u001B[A\n",
      " 74%|███████▎  | 7355/10000 [00:16<00:05, 450.15it/s]\u001B[A\n",
      " 74%|███████▍  | 7401/10000 [00:16<00:05, 451.69it/s]\u001B[A\n",
      " 75%|███████▍  | 7452/10000 [00:16<00:05, 465.92it/s]\u001B[A\n",
      " 75%|███████▍  | 7499/10000 [00:16<00:05, 448.75it/s]\u001B[A\n",
      " 75%|███████▌  | 7545/10000 [00:16<00:05, 427.51it/s]\u001B[A\n",
      " 76%|███████▌  | 7589/10000 [00:17<00:05, 414.80it/s]\u001B[A\n",
      " 76%|███████▋  | 7631/10000 [00:17<00:05, 407.22it/s]\u001B[A\n",
      " 77%|███████▋  | 7674/10000 [00:17<00:05, 412.42it/s]\u001B[A\n",
      " 77%|███████▋  | 7716/10000 [00:17<00:05, 408.75it/s]\u001B[A\n",
      " 78%|███████▊  | 7760/10000 [00:17<00:05, 416.50it/s]\u001B[A\n",
      " 78%|███████▊  | 7802/10000 [00:17<00:05, 413.91it/s]\u001B[A\n",
      " 78%|███████▊  | 7846/10000 [00:17<00:05, 417.84it/s]\u001B[A\n",
      " 79%|███████▉  | 7892/10000 [00:17<00:04, 427.64it/s]\u001B[A\n",
      " 79%|███████▉  | 7937/10000 [00:17<00:04, 432.92it/s]\u001B[A\n",
      " 80%|███████▉  | 7986/10000 [00:17<00:04, 449.69it/s]\u001B[A\n",
      " 80%|████████  | 8032/10000 [00:18<00:04, 433.53it/s]\u001B[A\n",
      " 81%|████████  | 8080/10000 [00:18<00:04, 446.92it/s]\u001B[A\n",
      " 81%|████████▏ | 8128/10000 [00:18<00:04, 456.57it/s]\u001B[A\n",
      " 82%|████████▏ | 8177/10000 [00:18<00:03, 462.32it/s]\u001B[A\n",
      " 82%|████████▏ | 8224/10000 [00:18<00:03, 461.90it/s]\u001B[A\n",
      " 83%|████████▎ | 8271/10000 [00:18<00:03, 456.19it/s]\u001B[A\n",
      " 83%|████████▎ | 8320/10000 [00:18<00:03, 463.37it/s]\u001B[A\n",
      " 84%|████████▎ | 8369/10000 [00:18<00:03, 468.47it/s]\u001B[A\n",
      " 84%|████████▍ | 8416/10000 [00:18<00:03, 456.80it/s]\u001B[A\n",
      " 85%|████████▍ | 8462/10000 [00:18<00:03, 437.42it/s]\u001B[A\n",
      " 85%|████████▌ | 8507/10000 [00:19<00:03, 438.50it/s]\u001B[A\n",
      " 86%|████████▌ | 8554/10000 [00:19<00:03, 446.26it/s]\u001B[A\n",
      " 86%|████████▌ | 8599/10000 [00:19<00:03, 440.96it/s]\u001B[A\n",
      " 86%|████████▋ | 8644/10000 [00:19<00:03, 437.23it/s]\u001B[A\n",
      " 87%|████████▋ | 8688/10000 [00:19<00:03, 435.50it/s]\u001B[A\n",
      " 87%|████████▋ | 8732/10000 [00:19<00:02, 431.78it/s]\u001B[A\n",
      " 88%|████████▊ | 8777/10000 [00:19<00:02, 435.83it/s]\u001B[A\n",
      " 88%|████████▊ | 8821/10000 [00:19<00:02, 431.25it/s]\u001B[A\n",
      " 89%|████████▊ | 8865/10000 [00:19<00:02, 427.56it/s]\u001B[A\n",
      " 89%|████████▉ | 8918/10000 [00:20<00:02, 457.43it/s]\u001B[A\n",
      " 90%|████████▉ | 8964/10000 [00:20<00:02, 451.58it/s]\u001B[A\n",
      " 90%|█████████ | 9010/10000 [00:20<00:02, 442.37it/s]\u001B[A\n",
      " 91%|█████████ | 9055/10000 [00:20<00:02, 443.30it/s]\u001B[A\n",
      " 91%|█████████ | 9100/10000 [00:20<00:02, 428.96it/s]\u001B[A\n",
      " 91%|█████████▏| 9144/10000 [00:20<00:02, 392.73it/s]\u001B[A\n",
      " 92%|█████████▏| 9194/10000 [00:20<00:01, 420.69it/s]\u001B[A\n",
      " 92%|█████████▏| 9237/10000 [00:20<00:01, 418.61it/s]\u001B[A\n",
      " 93%|█████████▎| 9280/10000 [00:20<00:01, 401.41it/s]\u001B[A\n",
      " 93%|█████████▎| 9323/10000 [00:20<00:01, 408.22it/s]\u001B[A\n",
      " 94%|█████████▎| 9371/10000 [00:21<00:01, 427.34it/s]\u001B[A\n",
      " 94%|█████████▍| 9415/10000 [00:21<00:01, 415.46it/s]\u001B[A\n",
      " 95%|█████████▍| 9466/10000 [00:21<00:01, 442.28it/s]\u001B[A\n",
      " 95%|█████████▌| 9511/10000 [00:21<00:01, 443.21it/s]\u001B[A\n",
      " 96%|█████████▌| 9556/10000 [00:21<00:01, 405.09it/s]\u001B[A\n",
      " 96%|█████████▌| 9598/10000 [00:21<00:01, 390.94it/s]\u001B[A\n",
      " 96%|█████████▋| 9644/10000 [00:21<00:00, 405.33it/s]\u001B[A\n",
      " 97%|█████████▋| 9686/10000 [00:21<00:00, 388.87it/s]\u001B[A\n",
      " 97%|█████████▋| 9728/10000 [00:21<00:00, 397.37it/s]\u001B[A\n",
      " 98%|█████████▊| 9770/10000 [00:22<00:00, 401.50it/s]\u001B[A\n",
      " 98%|█████████▊| 9811/10000 [00:22<00:00, 398.24it/s]\u001B[A\n",
      " 99%|█████████▊| 9856/10000 [00:22<00:00, 410.70it/s]\u001B[A\n",
      " 99%|█████████▉| 9898/10000 [00:22<00:00, 406.37it/s]\u001B[A\n",
      " 99%|█████████▉| 9939/10000 [00:22<00:00, 400.46it/s]\u001B[A\n",
      "100%|██████████| 10000/10000 [00:22<00:00, 440.38it/s][A\n"
     ]
    }
   ],
   "source": [
    "limit = 10_000\n",
    "\n",
    "df = pd.read_csv(DATA_PATH / 'raw' / 'ru-tweet-corp.csv', names=['text', 'label'], usecols=[4, 5])\n",
    "df = shuffle(df)\n",
    "\n",
    "X = df.text.head(limit)\n",
    "y = df.label.head(limit)\n",
    "\n",
    "X = X.progress_apply(tokenize)\n",
    "# y = y.progress_apply(lambda label: [label, 1 - label][::-1])\n",
    "\n",
    "X = np.array(X.tolist())\n",
    "y = np.array(y.tolist())\n",
    "\n",
    "X = torch.from_numpy(X)\n",
    "y = torch.from_numpy(y).float()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3642.96it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 3850.32it/s]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(DATA_PATH / 'raw' / 'clickbait' / 'train.csv')\n",
    "test = pd.read_csv(DATA_PATH / 'raw' / 'clickbait' / 'test.csv')\n",
    "\n",
    "train.replace(to_replace={'clickbait': 1., 'not-clickbait': 0.}, inplace=True)\n",
    "test.replace(to_replace={'clickbait': 1., 'not-clickbait': 0.}, inplace=True)\n",
    "\n",
    "# df = pd.concat([train, test])\n",
    "\n",
    "X_train = train.title.progress_apply(tokenize)\n",
    "X_test = test.title.progress_apply(tokenize)\n",
    "\n",
    "y_train = train.label\n",
    "y_test = test.label\n",
    "\n",
    "X_train = torch.from_numpy(np.array(X_train.tolist()))\n",
    "X_test = torch.from_numpy(np.array(X_test.tolist()))\n",
    "\n",
    "y_train = torch.from_numpy(np.array(y_train.tolist()))\n",
    "y_test = torch.from_numpy(np.array(y_test.tolist()))\n",
    "# y = df.label\n",
    "# y = y.progress_apply(lambda label: [label, 1 - label][::-1])\n",
    "\n",
    "# X = np.array(X.tolist())\n",
    "# y = np.array(y.tolist())\n",
    "#\n",
    "# X = torch.from_numpy(X)\n",
    "# y = torch.from_numpy(y).float()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(TensorDataset(X_train, y_train), batch_size=32)\n",
    "test_dataloader = DataLoader(TensorDataset(X_test, y_test), batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "model = TextCNN(\n",
    "    vocab_size=len(tokenizer.vectors),\n",
    "    embedding_dim=tokenizer.vectors.vector_size,\n",
    "    kernel_sizes=[10, 20, 30, 40],\n",
    "    num_filters=100,\n",
    "    num_classes=1,\n",
    "    d_prob=0.5,\n",
    "    embedding_weights=weights\n",
    ")\n",
    "\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "step = Step(model, criterion, optimizer)\n",
    "eval = Eval(model)\n",
    "\n",
    "trainer = Engine(step)\n",
    "train_evaluator = Engine(eval)\n",
    "validation_evaluator = Engine(eval)\n",
    "\n",
    "RunningAverage(output_transform=identity).attach(trainer, 'loss')\n",
    "\n",
    "Accuracy(lambda output: (torch.round(output[0]), output[1])).attach(train_evaluator, 'accuracy')\n",
    "Loss(criterion).attach(train_evaluator, 'bce')\n",
    "\n",
    "Accuracy(lambda output: (torch.round(output[0]), output[1])).attach(validation_evaluator, 'accuracy')\n",
    "Loss(criterion).attach(validation_evaluator, 'bce')\n",
    "\n",
    "progress_bar = ProgressBar(persist=True, bar_format='')\n",
    "progress_bar.attach(trainer, ['loss'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 1/250 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47105f8b4aca4deeb3677bfd11de7fb6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Engine run is terminating due to exception: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [175]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\uiqko\\pyenvs\\mint\\lib\\site-packages\\ignite\\engine\\engine.py:704\u001B[0m, in \u001B[0;36mEngine.run\u001B[1;34m(self, data, max_epochs, epoch_length, seed)\u001B[0m\n\u001B[0;32m    701\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch_length should be provided if data is None\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mdataloader \u001B[38;5;241m=\u001B[39m data\n\u001B[1;32m--> 704\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_internal_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\uiqko\\pyenvs\\mint\\lib\\site-packages\\ignite\\engine\\engine.py:783\u001B[0m, in \u001B[0;36mEngine._internal_run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    781\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataloader_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    782\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEngine run is terminating due to exception: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 783\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_exception\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    785\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataloader_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    786\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\n",
      "File \u001B[1;32mc:\\users\\uiqko\\pyenvs\\mint\\lib\\site-packages\\ignite\\engine\\engine.py:466\u001B[0m, in \u001B[0;36mEngine._handle_exception\u001B[1;34m(self, e)\u001B[0m\n\u001B[0;32m    464\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fire_event(Events\u001B[38;5;241m.\u001B[39mEXCEPTION_RAISED, e)\n\u001B[0;32m    465\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 466\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "File \u001B[1;32mc:\\users\\uiqko\\pyenvs\\mint\\lib\\site-packages\\ignite\\engine\\engine.py:753\u001B[0m, in \u001B[0;36mEngine._internal_run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    750\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataloader_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    751\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_setup_engine()\n\u001B[1;32m--> 753\u001B[0m time_taken \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_once_on_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    754\u001B[0m \u001B[38;5;66;03m# time is available for handlers but must be update after fire\u001B[39;00m\n\u001B[0;32m    755\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mtimes[Events\u001B[38;5;241m.\u001B[39mEPOCH_COMPLETED\u001B[38;5;241m.\u001B[39mname] \u001B[38;5;241m=\u001B[39m time_taken\n",
      "File \u001B[1;32mc:\\users\\uiqko\\pyenvs\\mint\\lib\\site-packages\\ignite\\engine\\engine.py:840\u001B[0m, in \u001B[0;36mEngine._run_once_on_dataset\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    838\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39miteration \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    839\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fire_event(Events\u001B[38;5;241m.\u001B[39mITERATION_STARTED)\n\u001B[1;32m--> 840\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39moutput \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstate\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    841\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fire_event(Events\u001B[38;5;241m.\u001B[39mITERATION_COMPLETED)\n\u001B[0;32m    843\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshould_terminate \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshould_terminate_single_epoch:\n",
      "File \u001B[1;32mD:\\projects\\nyaural_nyatworks\\nya_ml_research\\src\\step.py:18\u001B[0m, in \u001B[0;36mStep.__call__\u001B[1;34m(self, engine, batch)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     16\u001B[0m x, y \u001B[38;5;241m=\u001B[39m batch\n\u001B[1;32m---> 18\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion(y_pred, y\u001B[38;5;241m.\u001B[39mfloat())\n\u001B[0;32m     21\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32mc:\\users\\uiqko\\pyenvs\\mint\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\projects\\nyaural_nyatworks\\nya_ml_research\\src\\models\\ignitecnn.py:48\u001B[0m, in \u001B[0;36mTextCNN.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     45\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding(x)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     47\u001B[0m x \u001B[38;5;241m=\u001B[39m [F\u001B[38;5;241m.\u001B[39mrelu(conv(x)) \u001B[38;5;28;01mfor\u001B[39;00m conv \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv]\n\u001B[1;32m---> 48\u001B[0m x \u001B[38;5;241m=\u001B[39m [F\u001B[38;5;241m.\u001B[39mmax_pool1d(c, c\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39msqueeze(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m x]\n\u001B[0;32m     50\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(x, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     51\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(x))\n",
      "File \u001B[1;32mD:\\projects\\nyaural_nyatworks\\nya_ml_research\\src\\models\\ignitecnn.py:48\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     45\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding(x)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     46\u001B[0m \u001B[38;5;66;03m# print(x.size())\u001B[39;00m\n\u001B[1;32m---> 48\u001B[0m x \u001B[38;5;241m=\u001B[39m [F\u001B[38;5;241m.\u001B[39mrelu(\u001B[43mconv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;28;01mfor\u001B[39;00m conv \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv]\n\u001B[0;32m     49\u001B[0m \u001B[38;5;66;03m# print([x_.size() for x_ in x])\u001B[39;00m\n\u001B[0;32m     50\u001B[0m x \u001B[38;5;241m=\u001B[39m [F\u001B[38;5;241m.\u001B[39mmax_pool1d(c, c\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39msqueeze(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m x]\n",
      "File \u001B[1;32mc:\\users\\uiqko\\pyenvs\\mint\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mc:\\users\\uiqko\\pyenvs\\mint\\lib\\site-packages\\torch\\nn\\modules\\conv.py:302\u001B[0m, in \u001B[0;36mConv1d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    301\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 302\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\uiqko\\pyenvs\\mint\\lib\\site-packages\\torch\\nn\\modules\\conv.py:298\u001B[0m, in \u001B[0;36mConv1d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    294\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    295\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv1d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    296\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    297\u001B[0m                     _single(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 298\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    299\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer.run(train_dataloader, max_epochs=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "batch = next(iter(test_dataloader))\n",
    "y_pred = model(batch[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "data": {
      "text/plain": "'rnn_torchviz.png'"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "make_dot(y_pred, params=dict(list(model.named_parameters()))).render(\"rnn_torchviz\", format=\"png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}