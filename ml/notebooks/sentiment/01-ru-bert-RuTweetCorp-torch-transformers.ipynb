{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, RandomSampler\n",
    "from torch.optim.adamw import AdamW\n",
    "from transformers import BertForSequenceClassification, BertConfig, AutoTokenizer, AutoModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "from ml.src.data.load_datasets import load_RuTweetCorp, DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: EOF inside string starting at row 109353",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mParserError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[1;32mIn [8]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[0m data_path \u001B[38;5;241m=\u001B[39m \u001B[43mload_RuTweetCorp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconcat\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\projects\\nyaural_nyatworks\\ml\\src\\data\\load_datasets.py:24\u001B[0m, in \u001B[0;36mload_RuTweetCorp\u001B[1;34m(positive_link, negative_link, save_path, overwrite, concat)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m concat:\n\u001B[0;32m     23\u001B[0m     read_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(delimiter\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m;\u001B[39m\u001B[38;5;124m'\u001B[39m, header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m---> 24\u001B[0m     data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat((pd\u001B[38;5;241m.\u001B[39mread_csv(positive_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mread_kwargs), pd\u001B[38;5;241m.\u001B[39mread_csv(negative_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mread_kwargs)))\n\u001B[0;32m     25\u001B[0m     data\u001B[38;5;241m.\u001B[39mto_csv(path \u001B[38;5;241m:=\u001B[39m save_path \u001B[38;5;241m/\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mru-tweet-corp.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;66;03m# os.remove(negative_path)\u001B[39;00m\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;66;03m# os.remove(positive_path)\u001B[39;00m\n",
      "File \u001B[1;32mD:\\projects\\nyaural_nyatworks\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    306\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    307\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39marguments),\n\u001B[0;32m    308\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    309\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mstacklevel,\n\u001B[0;32m    310\u001B[0m     )\n\u001B[1;32m--> 311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\projects\\nyaural_nyatworks\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    665\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    666\u001B[0m     dialect,\n\u001B[0;32m    667\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    676\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[0;32m    677\u001B[0m )\n\u001B[0;32m    678\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 680\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\projects\\nyaural_nyatworks\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:581\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    578\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[0;32m    580\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[1;32m--> 581\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\projects\\nyaural_nyatworks\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1250\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m   1248\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[0;32m   1249\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1250\u001B[0m     index, columns, col_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1251\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1252\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32mD:\\projects\\nyaural_nyatworks\\venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:225\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    224\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[1;32m--> 225\u001B[0m         chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_low_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    226\u001B[0m         \u001B[38;5;66;03m# destructive to chunks\u001B[39;00m\n\u001B[0;32m    227\u001B[0m         data \u001B[38;5;241m=\u001B[39m _concatenate_chunks(chunks)\n",
      "File \u001B[1;32mD:\\projects\\nyaural_nyatworks\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:805\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mD:\\projects\\nyaural_nyatworks\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mD:\\projects\\nyaural_nyatworks\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:847\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mD:\\projects\\nyaural_nyatworks\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1960\u001B[0m, in \u001B[0;36mpandas._libs.parsers.raise_parser_error\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mParserError\u001B[0m: Error tokenizing data. C error: EOF inside string starting at row 109353"
     ]
    }
   ],
   "source": [
    "data_path = load_RuTweetCorp(concat=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# data_path = DATA_PATH / 'raw' / 'ru-tweet-corp.csv'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uiqko\\AppData\\Local\\Temp\\ipykernel_5500\\2784446461.py:1: DtypeWarning: Columns (3,4,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(data_path, header=None)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(data_path, header=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "data = shuffle(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "           0             1             2                3   \\\n207684  92774           NaN           NaN              NaN   \n41435   41435  4.099313e+17  1.386570e+09      xizydinivem   \n35198   35198  4.097734e+17  1.386533e+09  mihail_za_73034   \n59996   59996  4.101440e+17  1.386621e+09        zellenkov   \n179344  64434           NaN           NaN              NaN   \n180756  65846           NaN           NaN              NaN   \n15785   15785  4.093530e+17  1.386432e+09    ZzzCalifornia   \n197204  82294           NaN           NaN              NaN   \n202626  87716           NaN           NaN              NaN   \n2531     2531  4.089336e+17  1.386332e+09       JuliAussie   \n\n                                                       4    5   6   7   8   \\\n207684                                                NaN  NaN   0   0   0   \n41435   З новым годам родны край. I другiя края таксам...  1.0   0   0   0   \n35198   RT @anton_la_752289: кто то написал: \"ЛЮБИЛА е...  1.0   0   1   0   \n59996   @gasparcompany @xenia_sobchak воронины епть! К...  1.0   0   0   0   \n179344                                                NaN  NaN   0   0   0   \n180756                                                NaN  NaN   0   0   0   \n15785   интересно после грибов тольк мне весело?не зря...  1.0   0   0   0   \n197204                                                NaN  NaN   0   0   0   \n202626                                                NaN  NaN   0   0   0   \n2531    RT @Dance_with_me_: Кто бы знал как я не люблю...  1.0   0   1   0   \n\n             9   ...    12            13            14               15  \\\n207684      NaN  ...   NaN  4.222319e+17  1.389503e+09  darnellhphedgep   \n41435     531.0  ...   0.0           NaN           NaN              NaN   \n35198     872.0  ...   0.0           NaN           NaN              NaN   \n59996     779.0  ...   0.0           NaN           NaN              NaN   \n179344      NaN  ...   NaN  4.169686e+17  1.388248e+09  blueberries_kap   \n180756      NaN  ...   NaN  4.172103e+17  1.388306e+09    KateMaskevich   \n15785    2398.0  ...   1.0           NaN           NaN              NaN   \n197204      NaN  ...   NaN  4.198281e+17  1.388930e+09  KseniaIvantsova   \n202626      NaN  ...   NaN  4.212637e+17  1.389272e+09      lutikova_li   \n2531    36948.0  ...  15.0           NaN           NaN              NaN   \n\n                                                       16   17      18     19  \\\n207684       я хочу тебя найти....но не знаю где искать(  -1.0  3637.0  267.0   \n41435                                                 NaN  NaN     NaN    NaN   \n35198                                                 NaN  NaN     NaN    NaN   \n59996                                                 NaN  NaN     NaN    NaN   \n179344  @FatStupidDeer я ушел с дискоча нафиг, я еще т... -1.0  6286.0  787.0   \n180756       расстроена=( такое западло да под Новый Год! -1.0    17.0    4.0   \n15785                                                 NaN  NaN     NaN    NaN   \n197204  @DMC_SERGHO @Club_Relax_ Вот они эти мишки, ко... -1.0  4632.0   69.0   \n202626  @fedechka_m кстати что-то я нигде не смогла на... -1.0   691.0   18.0   \n2531                                                  NaN  NaN     NaN    NaN   \n\n           20    21  \n207684   35.0   0.0  \n41435     NaN   NaN  \n35198     NaN   NaN  \n59996     NaN   NaN  \n179344  455.0  13.0  \n180756    5.0   0.0  \n15785     NaN   NaN  \n197204   31.0   2.0  \n202626   23.0   0.0  \n2531      NaN   NaN  \n\n[10 rows x 22 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>207684</th>\n      <td>92774</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>4.222319e+17</td>\n      <td>1.389503e+09</td>\n      <td>darnellhphedgep</td>\n      <td>я хочу тебя найти....но не знаю где искать(</td>\n      <td>-1.0</td>\n      <td>3637.0</td>\n      <td>267.0</td>\n      <td>35.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>41435</th>\n      <td>41435</td>\n      <td>4.099313e+17</td>\n      <td>1.386570e+09</td>\n      <td>xizydinivem</td>\n      <td>З новым годам родны край. I другiя края таксам...</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>531.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>35198</th>\n      <td>35198</td>\n      <td>4.097734e+17</td>\n      <td>1.386533e+09</td>\n      <td>mihail_za_73034</td>\n      <td>RT @anton_la_752289: кто то написал: \"ЛЮБИЛА е...</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>872.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>59996</th>\n      <td>59996</td>\n      <td>4.101440e+17</td>\n      <td>1.386621e+09</td>\n      <td>zellenkov</td>\n      <td>@gasparcompany @xenia_sobchak воронины епть! К...</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>779.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>179344</th>\n      <td>64434</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>4.169686e+17</td>\n      <td>1.388248e+09</td>\n      <td>blueberries_kap</td>\n      <td>@FatStupidDeer я ушел с дискоча нафиг, я еще т...</td>\n      <td>-1.0</td>\n      <td>6286.0</td>\n      <td>787.0</td>\n      <td>455.0</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>180756</th>\n      <td>65846</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>4.172103e+17</td>\n      <td>1.388306e+09</td>\n      <td>KateMaskevich</td>\n      <td>расстроена=( такое западло да под Новый Год!</td>\n      <td>-1.0</td>\n      <td>17.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>15785</th>\n      <td>15785</td>\n      <td>4.093530e+17</td>\n      <td>1.386432e+09</td>\n      <td>ZzzCalifornia</td>\n      <td>интересно после грибов тольк мне весело?не зря...</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2398.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>197204</th>\n      <td>82294</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>4.198281e+17</td>\n      <td>1.388930e+09</td>\n      <td>KseniaIvantsova</td>\n      <td>@DMC_SERGHO @Club_Relax_ Вот они эти мишки, ко...</td>\n      <td>-1.0</td>\n      <td>4632.0</td>\n      <td>69.0</td>\n      <td>31.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>202626</th>\n      <td>87716</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>4.212637e+17</td>\n      <td>1.389272e+09</td>\n      <td>lutikova_li</td>\n      <td>@fedechka_m кстати что-то я нигде не смогла на...</td>\n      <td>-1.0</td>\n      <td>691.0</td>\n      <td>18.0</td>\n      <td>23.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2531</th>\n      <td>2531</td>\n      <td>4.089336e+17</td>\n      <td>1.386332e+09</td>\n      <td>JuliAussie</td>\n      <td>RT @Dance_with_me_: Кто бы знал как я не люблю...</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>36948.0</td>\n      <td>...</td>\n      <td>15.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 22 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "X, y = data.iloc[:, 4], data.iloc[:, 5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "X.dropna(inplace=True)\n",
    "y.dropna(inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "41435    З новым годам родны край. I другiя края таксам...\n35198    RT @anton_la_752289: кто то написал: \"ЛЮБИЛА е...\n59996    @gasparcompany @xenia_sobchak воронины епть! К...\n15785    интересно после грибов тольк мне весело?не зря...\n2531     RT @Dance_with_me_: Кто бы знал как я не люблю...\n43926    Завтра последний рывок иии.. Свободааа\\r\\nПрав...\n36726    У моей маленькой 3-родной сестренки, которой н...\n97400    Сижу на олимпиаде по физической культуре)\\r\\nН...\n35467    четкость этого фото просто зашкаливает:DDD htt...\n89994    Он должен быть крутым, я уверен!:-D http://t.c...\nName: 4, dtype: object"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "41435    1.0\n35198    1.0\n59996    1.0\n15785    1.0\n2531     1.0\n43926    1.0\n36726    1.0\n97400    1.0\n35467    1.0\n89994    1.0\nName: 5, dtype: float64"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "clean_tweet = partial(re.sub, '@\\w+')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', one_hot_labels=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "'Да, все-таки он немного похож на него. Но мой мальчик все равно лучше:D'"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "['Да',\n ',',\n 'все',\n '-',\n 'таки',\n 'он',\n 'немного',\n 'похож',\n 'на',\n 'него',\n '.',\n 'Но',\n 'мой',\n 'мальчик',\n 'все',\n 'равно',\n 'лучше',\n ':',\n 'D']"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(X[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "[14912,\n 128,\n 4752,\n 130,\n 24541,\n 2886,\n 17042,\n 15833,\n 1469,\n 7268,\n 132,\n 8312,\n 28635,\n 28677,\n 4752,\n 17561,\n 17874,\n 156,\n 176]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "85.43346967191715"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.map(len).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for x in X:\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        x,\n",
    "        add_special_tokens=True,\n",
    "        max_length=100,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids.append(encoded['input_ids'])\n",
    "    attention_masks.append(encoded['attention_mask'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  101,   791, 15485,  ...,     0,     0,     0],\n        [  101, 41286,   168,  ...,     0,     0,     0],\n        [  101,   168, 12399,  ...,     0,     0,     0],\n        ...,\n        [  101,   168, 31106,  ...,     0,     0,     0],\n        [  101,   168, 59726,  ...,     0,     0,     0],\n        [  101,   168, 20954,  ...,     0,     0,     0]])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "labels = torch.tensor(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "'Да, все-таки он немного похож на него. Но мой мальчик все равно лучше:D'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([  101,   791, 15485, 16170,  3687,  1468, 11172,   132,   186,  3487,\n          249,   878, 11744, 37831, 10338,   855, 15485, 16170,   122,   102,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1., dtype=torch.float64)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.9)\n",
    "val_size = len(dataset) - train_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(dataset, (train_size, val_size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=RandomSampler(val_dataset),\n",
    "    batch_size=batch_size\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\n",
    "    'DeepPavlov/rubert-base-cased',\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 199 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "embeddings.word_embeddings.weight                       (119547, 768)\n",
      "embeddings.position_embeddings.weight                     (512, 768)\n",
      "embeddings.token_type_embeddings.weight                     (2, 768)\n",
      "embeddings.LayerNorm.weight                                   (768,)\n",
      "embeddings.LayerNorm.bias                                     (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "encoder.layer.0.attention.self.query.weight               (768, 768)\n",
      "encoder.layer.0.attention.self.query.bias                     (768,)\n",
      "encoder.layer.0.attention.self.key.weight                 (768, 768)\n",
      "encoder.layer.0.attention.self.key.bias                       (768,)\n",
      "encoder.layer.0.attention.self.value.weight               (768, 768)\n",
      "encoder.layer.0.attention.self.value.bias                     (768,)\n",
      "encoder.layer.0.attention.output.dense.weight             (768, 768)\n",
      "encoder.layer.0.attention.output.dense.bias                   (768,)\n",
      "encoder.layer.0.attention.output.LayerNorm.weight             (768,)\n",
      "encoder.layer.0.attention.output.LayerNorm.bias               (768,)\n",
      "encoder.layer.0.intermediate.dense.weight                (3072, 768)\n",
      "encoder.layer.0.intermediate.dense.bias                      (3072,)\n",
      "encoder.layer.0.output.dense.weight                      (768, 3072)\n",
      "encoder.layer.0.output.dense.bias                             (768,)\n",
      "encoder.layer.0.output.LayerNorm.weight                       (768,)\n",
      "encoder.layer.0.output.LayerNorm.bias                         (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "encoder.layer.11.output.LayerNorm.weight                      (768,)\n",
      "encoder.layer.11.output.LayerNorm.bias                        (768,)\n",
      "pooler.dense.weight                                       (768, 768)\n",
      "pooler.dense.bias                                             (768,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(timedelta(seconds=elapsed_rounded))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x225a9c87050>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_val = 42\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "train_stats = []\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0, # Default value in run_glue.py\n",
    "    num_training_steps=len(dataset) * epochs\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10\n",
      "Training...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [43]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     17\u001B[0m mask \u001B[38;5;241m=\u001B[39m mask\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     18\u001B[0m labels_ \u001B[38;5;241m=\u001B[39m labels_\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m---> 20\u001B[0m loss, logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[43mids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels_\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m total_train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     29\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32mD:\\projects\\nyaural_nyatworks\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "\u001B[1;31mTypeError\u001B[0m: forward() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "total_t0 = time.time()\n",
    "correct = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f'Epoch {epoch} / {epochs}')\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        model.zero_grad()\n",
    "        ids, mask, labels_ = batch\n",
    "\n",
    "        ids = ids.to(device)\n",
    "        mask = mask.to(device)\n",
    "        labels_ = labels_.to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=mask,\n",
    "        )\n",
    "\n",
    "        pred = torch.argmax(outputs.logits, dim=1)\n",
    "        loss = loss_fn(pred, labels_)\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        correct += torch.sum(pred == labels_)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = np.average(total_train_loss)\n",
    "\n",
    "    train_time = format_time(time.time() - t0)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(train_time))\n",
    "\n",
    "    # validation\n",
    "    print('validation...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    eval_steps = 0\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        ids, mask, labels_ = batch\n",
    "        print(ids.size, mask.size, labels.size)\n",
    "        ids = ids.to(device)\n",
    "        mask = mask.to(device)\n",
    "        labels_ = labels_.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=mask,\n",
    "            )\n",
    "\n",
    "            pred = torch.argmax(outputs.logits, dim=1)\n",
    "            loss = loss_fn(pred, labels_)\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "        # logits = logits.\n",
    "        total_eval_accuracy += accuracy_score(logits, labels)\n",
    "\n",
    "    avg_val_accuracy = np.average(total_eval_accuracy)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    avg_val_loss = np.average(total_eval_loss)\n",
    "\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    train_stats.append({\n",
    "        'epoch': epoch,\n",
    "        'train loss': avg_train_loss,\n",
    "        'valid loss': avg_val_loss,\n",
    "        'valid accuracy': avg_val_accuracy,\n",
    "        'train time': train_time,\n",
    "        'validation time': validation_time,\n",
    "    })\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}